{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GradientBasedAttackDVS128Gesture.ipynb","provenance":[],"collapsed_sections":["mF7QOb-0g9Xz","0nJphReWMz21","e1grpPmF4Bqg"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mF7QOb-0g9Xz"},"source":["# Install SlayerPytorch on Colab\n","After the installations the runtime needs to be restarted. \n","```\n","exit()\n","```\n","Will restart the runtime without deleting files. The runtime will automatically start. And if you press \"run all\" the run is not interrupted and works till the end."]},{"cell_type":"code","metadata":{"id":"aSnbF5w-jTGQ"},"source":["!git clone https://github.com/bamsumit/slayerPytorch\n","!pip install ninja\n","exit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhdpSnPWkG8r"},"source":["%cd slayerPytorch/\n","!python setup.py install\n","exit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7F0vk1qah9Dh"},"source":["Test to verify if everything went well with the installation"]},{"cell_type":"code","metadata":{"id":"35lkjdNJl20b"},"source":["%cd slayerPytorch/test/\n","!python -m  unittest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nJphReWMz21"},"source":["# SNNs configuration and checkpoint loading"]},{"cell_type":"code","metadata":{"id":"1XuvMfOvpf_I"},"source":["!unzip -q '/content/drive/My Drive/Test.zip' -d /content/ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGmhuRpatzVK"},"source":["import sys, os\n","CURRENT_TEST_DIR = os.getcwd()\n","sys.path.append(CURRENT_TEST_DIR + \"/../../src\")\n","\n","from datetime import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import slayerSNN as snn\n","#from learningStats import learningStats\n","from IPython.display import HTML\n","import time\n","import shutil\n","\n","def save_ckp(state, is_best_loss, is_best_acc, checkpoint_dir, best_model_dir):\n","    f_path = checkpoint_dir+'checkpoint.pt'\n","    torch.save(state, f_path)\n","    if is_best_loss:\n","        best_fpath = best_model_dir+'best_model.pt'\n","        shutil.copyfile(f_path, best_fpath)\n","    if is_best_acc:\n","        acc_fpath =  best_model_dir+'best_acc.pt'\n","        shutil.copyfile(f_path, acc_fpath)\n","        \n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']\n","\n","actionName = [\n","    'hand_clapping',\n","    'right_hand_wave',\n","    'left_hand_wave',\n","    'right_arm_clockwise',\n","    'right_arm_counter_clockwise',\n","    'left_arm_clockwise', \n","    'left_arm_counter_clockwise',\n","    'arm_roll',\n","    'air_drums',\n","    'air_guitar',\n","    'other_gestures',\n","]\n","\n","# Define dataset module\n","class IBMGestureDataset(Dataset):\n","    def __init__(self, datasetPath, sampleFile, samplingTime, sampleLength):\n","        self.path = datasetPath \n","        self.samples = np.loadtxt(sampleFile).astype('int')\n","        self.samplingTime = samplingTime\n","        self.nTimeBins    = int(sampleLength / samplingTime)\n","\n","    def __getitem__(self, index):\n","        # Read inoput and label\n","        inputIndex  = self.samples[index, 0]\n","        classLabel  = self.samples[index, 1]\n","        # Read input spike\n","        inputSpikes = snn.io.readNpSpikes(\n","                        self.path + str(inputIndex.item()) + '.npy'\n","                        ).toSpikeTensor(torch.zeros((2,128,128,self.nTimeBins)),\n","                        samplingTime=self.samplingTime)\n","        # Create one-hot encoded desired matrix\n","        desiredClass = torch.zeros((11, 1, 1, 1))\n","        desiredClass[classLabel,...] = 1\n","        \n","        return inputSpikes, desiredClass, classLabel\n","\n","    def __len__(self):\n","        return self.samples.shape[0]\n","\t\t\n","# Define the network\n","class Network(torch.nn.Module):\n","    def __init__(self, netParams):\n","        super(Network, self).__init__()\n","        # initialize slayer\n","        slayer = snn.loihi(netParams['neuron'], netParams['simulation'])\n","        self.slayer = slayer\n","        # define network functions\n","        self.conv1 = slayer.conv(2, 16, 5, padding=2, weightScale=10)\n","        self.conv2 = slayer.conv(16, 32, 3, padding=1, weightScale=50)\n","        self.pool1 = slayer.pool(4)\n","        self.pool2 = slayer.pool(2)\n","        self.pool3 = slayer.pool(2)\n","        self.fc1   = slayer.dense((8*8*32), 512)\n","        self.fc2   = slayer.dense(512, 11)\n","        self.drop  = slayer.dropout(0.1)\n","\n","    def forward(self, spikeInput):\n","        spike = self.slayer.spikeLoihi(self.pool1(spikeInput )) # 32, 32, 2\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.drop(spike)\n","        spike = self.slayer.spikeLoihi(self.conv1(spike)) # 32, 32, 16\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.slayer.spikeLoihi(self.pool2(spike)) # 16, 16, 16\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.drop(spike)\n","        spike = self.slayer.spikeLoihi(self.conv2(spike)) # 16, 16, 32\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.slayer.spikeLoihi(self.pool3(spike)) #  8,  8, 32\n","        spike = spike.reshape((spike.shape[0], -1, 1, 1, spike.shape[-1]))\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.drop(spike)\n","        spike = self.slayer.spikeLoihi(self.fc1  (spike)) # 512\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        spike = self.slayer.spikeLoihi(self.fc2  (spike)) # 11\n","        spike = self.slayer.delayShift(spike, 1)\n","        \n","        return spike\n","\n","\n","if __name__ == '__main__':\n","\tnetParams = snn.params('/content/slayerPytorch/exampleLoihi/03_IBMGesture/network.yaml')\n","\t\n","\t# Define the cuda device to run the code on.\n","\tdevice = torch.device('cuda')\n","\t# deviceIds = [2, 3]\n","\n","\t# Create network instance.\n","\tnet = Network(netParams).to(device)\n","\t# net = torch.nn.DataParallel(Network(netParams).to(device), device_ids=deviceIds)\n","\n","\t# Create snn loss instance.\n","\terror = snn.loss(netParams, snn.loihi).to(device)\n","\n","\t# Define optimizer module.\n","\t# optimizer = torch.optim.Adam(net.parameters(), lr = 0.01, amsgrad = True)\n","\toptimizer = snn.utils.optim.Nadam(net.parameters(), lr = 0.01, amsgrad = True)\n","\n","\t#Dataset and dataLoader instances.\n","\n","\ttestingSet = IBMGestureDataset(datasetPath =\"/content/drive/MyDrive/Test/\", \n","\t\t\t\t\t\t\t\t\tsampleFile  = \"/content/drive/MyDrive/Test/test.txt\",\n","\t\t\t\t\t\t\t\t\tsamplingTime= 1.0,\n","\t\t\t\t\t\t\t\t\tsampleLength= 1450)\n","\ttestLoader = DataLoader(dataset=testingSet, batch_size=1, shuffle=False, num_workers=1)\n","  \n","\ti=0\n","\n","\t# Learning stats instance.\n","\tstats = snn.utils.stats()\n","\tckp_path = \"drive/My Drive/best/check.pt\"\n","\tnet, optimizer, start_epoch = load_ckp(ckp_path, net, optimizer)\n","\tstart_epoch\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCWG0atWNSrp"},"source":["#Gradient Based Attack"]},{"cell_type":"code","metadata":{"id":"N2ed6a14Fv7o"},"source":["testingSet = IBMGestureDataset(datasetPath  =  '/content/Test/', \n","                              sampleFile  ='/content/Test/test.txt',\n","                              samplingTime=1.0,\n","                              sampleLength=1450)\n","testLoader = DataLoader(dataset=testingSet, batch_size=1, shuffle=False, num_workers=4)\n","\n","import argparse\n","import numpy as np\n","import pdb \n","import torch, torchvision\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","\n","def update(x,y,t,s,A):\n","  for i in range(x-s,x+s+1):\n","     for j in range(y-s,y+s+1):\n","        if not(i==x and j==y) and i>0 and j>0 and i<128 and j<128:\n","\t        A[i][j] = t\n","    \n","\n","#Select the device to run the code \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print ('Device: ' + str(device))\n","\n","def calc_gradients(\n","\t\ttest_dataloader,\n","\t\tmodel,\n","\t\tmax_iter,\n","\t\tlearning_rate,\n","    samples,\n","\t\tfilter_on,\n","\t\ts,\n","\t\tT,\n","\t\ttargets=None,\n","\t\tweight_loss2=1,\n","\t\tbatch_size=1,\n","\t\tseq_len=40,):\n","\t\n","\t#Define the modifier and the optimizer\n","\n","  min_loss = 1e-5\n","  prev_loss = 1e-5\n","  correct = np.zeros(max_iter)\t\n","  for batch_index, (input_image, input_target, input_label) in enumerate(test_dataloader):\n","\n","    modif = torch.Tensor(1, seq_len, 2, 128, 128).fill_(1).to(device)\n","    modifier = torch.nn.Parameter(modif, requires_grad=True)\n","    optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n","\n","    if batch_index>samples:\n","      break\n","    model.eval()\n","    with torch.no_grad():\t\n","      input_image, input_label = input_image.to(device), input_label.to(device)\n","\n","    #Clean video prediction \n","    print(f'Batch Number: {batch_index}/{len(test_dataloader)}')\n","    print('------------------prediction for clean video-------------------')\n","    input_image = Variable(input_image, requires_grad=True)\n","    output = model.forward(input_image)\n","    pre_label= snn.predict.getClass(output)\n","    p = torch.nn.Softmax(dim=1)\t\t\n","    print (f'Prediction: {pre_label}, Original_label: {input_label.cpu().numpy()}')\n","\n","    print('------------------prediction for adversarial video-------------------')\n","\n","    min_in = input_image.min().detach() #0\n","    max_in = input_image.max().detach() #1\n","\n","    for iiter in range(max_iter):\n","      with open ('/content/modifier.txt','w') as file:\n","        file.write(str(modifier))      \n","      print(modifier[0,0,0,0,0])\t\t\t\n","      input_image = Variable(input_image, requires_grad=True)\n","      #model.lstm.reset_hidden_state()\n","      \n","      #Frames to be perturbed\n","      indicator = [0]*1450\n","      for i,x in enumerate(indicator):\n","        if (i>399 and i<510) or (i>724 and i<870) or (i>1199 and i< 1275):\n","          x=1\n","    \n","      #Perturbating the frames\n","\n","      input_image=torch.reshape(input_image, (1,1450,2,128,128)) ###\n","      true_image = modifier[0,0,:,:,:]+input_image[0,0,:,:,:]\n","      true_image = torch.unsqueeze(true_image, 0)\n","      \n","      for ll in range(seq_len-1): #seq_len =1450\n","        if indicator[ll+1] == 1:\n","          mask_temp = modifier[0,ll+1,:,:,:]+input_image[0,ll+1,:,:,:]\n","        else:\n","          mask_temp = input_image[0,ll+1,:,:,:]\n","        mask_temp = torch.unsqueeze(mask_temp,0)\n","        true_image = torch.cat((true_image, mask_temp),0)\n","\n","      #######\t\t Filter section \t######\n","      if filter_on :\n","        img = torch.reshape(true_image, (2,128,128,1450)).cpu().detach()\n","        TD = snn.io.spikeArrayToEvent(img.numpy(), samplingTime=1)\n","        snn.io.encodeNpSpikes(\"/content/drive/My Drive/temp/img.npy\", TD)\n","        data= np.load('/content/drive/My Drive/temp/img.npy'.format(i))\n","        data= data[data[:,3].argsort()] #sort elements by timestamp\n","        temp=np.zeros((128,128))\n","        real=[]\n","        for d in data:\n","          update(int(d[0]),int(d[1]),d[3],s,temp)\n","          if d[3]-temp[int(d[0])][int(d[1])]<T:\n","            real.append(d)\n","        real=np.stack(real, axis=0 )\n","        with open(\"/content/drive/My Drive/temp/fil.npy\", \"wb\") as f:\n","          np.save(f,real)\n","        true_image = snn.io.readNpSpikes(\n","            \"/content/drive/My Drive/temp/fil.npy\"\n","            ).toSpikeTensor(torch.zeros((2,128,128,1450)),\n","            samplingTime=1.0)\n","        true_image =  true_image.to(device)\n","      true_image = torch.reshape(true_image, (1,1450,2,128,128))\t\t\t\n","      #######\t\t End Filter Section \t #######\t\t\n","     \n","      #Prediction on the adversarial video\n","      output = model.forward(torch.reshape(true_image, (1,2,128,128,1450)))\n","      pre_label = snn.predict.getClass(output)\n","      numSpikes = torch.sum(output, 4, keepdim=True)\n","      print (f'numSpikes: {numSpikes[0,:,0,0]}, maxSpikes : {torch.max(numSpikes)}')\n","      numSpikes= torch.div(numSpikes,torch.max(numSpikes))\t\n","      probs= p(numSpikes.reshape((numSpikes.shape[0], -1)))\t\n","\n","      #extracting the probability of true label \n","      zero_array = torch.zeros(11).to(device) \n","      zero_array[input_label.cpu()] = 1\n","      true_label_onehot = probs*zero_array\n","      true_label_prob = torch.sum(true_label_onehot, 1)\n","\n","      if pre_label.numpy()==input_label.cpu().numpy() : correct[iiter]+=1\n","\n","      #Loss\n","      if targets is None:\n","        loss1 = -torch.log(1 - true_label_prob + 1e-6)\n","      else:\n","        loss1 = -torch.log(true_label_prob + 1e-6)\n","      loss1 = torch.mean(loss1)\n","      input_image=torch.reshape(input_image, (1,1450,2,128,128))\n","      loss2 =  weight_loss2*torch.sum(torch.sqrt(torch.mean(torch.pow((true_image-input_image+1e-21), 2), dim=0).mean(dim=2).mean(dim=2).mean(dim=1)))\n","      loss = loss1 +  loss2\n","      optimizer.zero_grad()\n","      loss.backward() \n","      optimizer.step()\n","      if True: #iiter % max_iter == 0: \n","        print (f'Probability for ground truth label : {true_label_prob.detach().cpu().numpy()}')\n","        if prev_loss < loss : \n","          print(f'Iteration: [{iiter+1}/{max_iter}], Loss: {loss}(\\u25b2), Loss1: {loss1}, Loss2: {loss2}\\n\\n')\n","        elif prev_loss > loss: \n","          print(f'Iteration: [{iiter+1}/{max_iter}], Loss: {loss}(\\u25bc), Loss1: {loss1}, Loss2: {loss2}\\n\\n')\n","        else: \n","          print(f'Iteration: [{iiter+1}/{max_iter}], Loss: {loss}, Loss1: {loss1}, Loss2: {loss2}\\n\\n')\n","      prev_loss = loss\n","\n","      break_condition = False\n","      if loss < min_loss:\n","        if torch.abs(loss-min_loss) < 0.0001:\n","            break_condition = True\n","            print ('Aborting early!')\n","        min_loss = loss\n","\n","      # Empty cache\n","      if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    # Save adversarial dataset as spike events\n","    image = torch.reshape(true_image, (2,128,128,1450)).cpu().detach()\n","    TD = snn.io.spikeArrayToEvent(image.numpy(), samplingTime=1)\n","    snn.io.encodeNpSpikes(\"/content/prova/{}.npy\".format(batch_index), TD)\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\t\n","  print(correct)\n","\n","\n","def main(net):\n","\n","  args = {\n","      'description': 'Sparse Adversarial Perturbations',\n","      'num_iter': 5,\n","      'learning_rate': 1,\n","      'samples': 264,\n","\t\t\t'filter_on': False,\n","\t\t\t's': 2,\n","\t\t\t'T': 5,\n","      'target': None,\n","      'weight_loss2': 0,\n","      'split_path': None,\n","      'split_number': 1,\n","      'img_dim': 128,\n","      'channels': 2,\n","\t\t\t}\n","  seq_len = 1450 #number of frames in a video\n","  batch_size = 1\n","  targets = None\n","  image_shape = (args['channels'], args['img_dim'], args['img_dim'])\n","  net = net.to(device)\n","  net.train()\n","  print('Model Loaded Successfully!')\n","\t#Call the function to generate the adversarial videos\n","  calc_gradients(\n","\ttestLoader,\n","\tnet,\n","\targs['num_iter'],\n","\targs['learning_rate'],\n","  args['samples'],\n","\targs['filter_on'],\n","\targs['s'],\n","\targs['T'],\n","\ttargets,\n","\targs['weight_loss2'],\n","\tbatch_size,\n","\tseq_len,\n","\t)\n","\n","if __name__ == '__main__':\n","\tmain(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXrEqIcs1vBE"},"source":["#Background Activity Filters\n","Several BAFs working on the adversarial examples. "]},{"cell_type":"code","metadata":{"id":"WG7PPDpZZOHG"},"source":["import numpy as np\n","\n","def update(x,y,t,A,s):\n","  for i in range(x-s,x+s+1):\n","     for j in range(y-s,y+s+1):\n","        if not(i==x and j==y) and i>=1 and j>=1 and i<127 and j<127:\n","\t        A[i][j] = t\n","for s in range(1,4):\n","  for i in range(264):         \n","    data= np.load('drive/My Drive/adv/{}.npy'.format(i))\n","    data= data[data[:,3].argsort()] #sort elements by timestamp\n","    temp=np.zeros((128,128))\n","    real1=[]\n","    real5=[]\n","    real10=[]\n","    real50=[]\n","    real100=[]\n","    real500=[]\n","    for d in data:\n","      update(int(d[0]),int(d[1]),d[3],temp,s)\n","      if d[3]-temp[int(d[0])][int(d[1])]<1:\n","        real1.append(d)\n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<5:\n","        real5.append(d)   \n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<10:\n","        real10.append(d)\n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<50:\n","        real50.append(d)  \n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<100:\n","        real100.append(d) \n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<500:\n","        real500.append(d) \n","\n","    real1=np.stack(real1, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,1,i), \"wb\") as f:\n","      np.save(f,real1)\n","\n","    real5=np.stack(real5, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,5,i), \"wb\") as f:\n","      np.save(f,real5)\n","\n","    real10=np.stack(real10, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,10,i), \"wb\") as f:\n","      np.save(f,real10)\n","\n","    real50=np.stack(real50, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,50,i), \"wb\") as f:\n","      np.save(f,real50)\n","\n","    real100=np.stack(real100, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,100,i), \"wb\") as f:\n","      np.save(f,real100)\n","\n","    real500=np.stack(real500, axis=0 )\n","    with open(\"/content/s{}_t{}/{}.npy\".format(s,500,i), \"wb\") as f:\n","      np.save(f,real500)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1grpPmF4Bqg"},"source":["#Test \n","Choose the desired Dataset (Normal, Attacked, Filtered) to verify the results."]},{"cell_type":"code","metadata":{"id":"pnl5Xu_h9LKI"},"source":["testSet = IBMGestureDataset(datasetPath  =  '/content/Test/', \n","                              sampleFile  ='/content/Test/test.txt',\n","                              samplingTime=1.0,\n","                              sampleLength=1450)\n","testLoader = DataLoader(dataset=filSet, batch_size=1, shuffle=False, num_workers=4)\n","\n","for epoch in range(1):\n","    stats.testing.reset()\n","    tSt = datetime.now()\n","    # Testing loop.\n","    for i, (input, target, label) in enumerate(testLoader, 0):\n","      net.eval()\n","      with torch.no_grad():\n","        input  = input.to(device)\n","        target = target.to(device) \n","    \n","      output = net.forward(input)\n","      stats.testing.correctSamples += torch.sum( snn.predict.getClass(output) == label ).data.item()\n","      stats.testing.numSamples     += len(label)\n","\n","      loss = error.numSpikes(output, target)\n","      stats.testing.lossSum += loss.cpu().data.item()\n","      stats.print(epoch, i)\n"],"execution_count":null,"outputs":[]}]}