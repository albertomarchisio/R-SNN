{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"NMNISTGradientBasedAttack.ipynb","provenance":[],"collapsed_sections":["mF7QOb-0g9Xz","YdbSqijZJ59a","YpDnqcJzEqKV"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mF7QOb-0g9Xz"},"source":["# Install SlayerPytorch on Colab\n","After the installations the runtime needs to be restarted. \n","```\n","exit()\n","```\n","Will restart the runtime without deleting files. The runtime will automatically start. And if you press \"run all\" the run is not interrupted and works till the end."]},{"cell_type":"code","metadata":{"id":"aSnbF5w-jTGQ"},"source":["!git clone https://github.com/bamsumit/slayerPytorch\n","!pip install ninja\n","exit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhdpSnPWkG8r"},"source":["%cd slayerPytorch/\n","!python setup.py install\n","exit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7F0vk1qah9Dh"},"source":["Test to verify if everything went well with the installation"]},{"cell_type":"code","metadata":{"id":"35lkjdNJl20b"},"source":["%cd slayerPytorch/test/\n","!python -m  unittest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YdbSqijZJ59a"},"source":["# SNN configuration"]},{"cell_type":"code","metadata":{"id":"d9bK2WCdKBLR"},"source":["import sys, os\n","CURRENT_TEST_DIR = os.getcwd()\n","sys.path.append(CURRENT_TEST_DIR + \"/../../src\")\n","\n","from datetime import datetime\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import slayerSNN as snn\n","import zipfile\n","import shutil\n","\n","# Read SNN configuration from yaml file\n","netParams = snn.params('/content/slayerPytorch/exampleLoihi/02_NMNIST/network.yaml')\n","\n","# Ts   = netParams['simulation']['Ts']\n","# Ns   = int(netParams['simulation']['tSample'] / netParams['simulation']['Ts'])\n","# Nin  = int(netParams['layer'][0]['dim'])\n","# Nhid = int(netParams['layer'][1]['dim'])\n","# Nout = int(netParams['layer'][2]['dim'])\n","\n","\t\t\t\n","# Define dataset module\n","class nmnistDataset(Dataset):\n","    def __init__(self, datasetPath, sampleFile, samplingTime, sampleLength):\n","        self.path = datasetPath \n","        self.samples = np.loadtxt(sampleFile).astype('int')\n","        self.samplingTime = samplingTime\n","        self.nTimeBins    = int(sampleLength / samplingTime)\n","\n","    def __getitem__(self, index):\n","        # Read inoput and label\n","        inputIndex  = self.samples[index, 0]\n","        classLabel  = self.samples[index, 1]\n","        # Read input spike\n","        inputSpikes = snn.io.readNpSpikes(\n","                        self.path + str(inputIndex.item()) + '.npy'\n","                        ).toSpikeTensor(torch.zeros((2,34,34,self.nTimeBins)),\n","                        samplingTime=self.samplingTime)\n","        # Create one-hot encoded desired matrix\n","        desiredClass = torch.zeros((10, 1, 1, 1))\n","        desiredClass[classLabel,...] = 1\n","        # Input spikes are reshaped to ignore the spatial dimension and the neurons are placed in channel dimension.\n","        # The spatial dimension can be maintained and used as it is.\n","        # It requires different definition of the dense layer.\n","        return inputSpikes, desiredClass, classLabel\n","        #return inputSpikes.reshape((-1, 1, 1, inputSpikes.shape[-1])), desiredClass, classLabel\n","\n","    def __len__(self):\n","        return self.samples.shape[0]\n","\n","# Define the network\n","class Network(torch.nn.Module):\n","    def __init__(self, netParams):\n","        super(Network, self).__init__()\n","        # initialize slayer\n","        slayer = snn.loihi(netParams['neuron'], netParams['simulation'])\n","        self.slayer = slayer\n","        # define network functions\n","        self.fc1   = slayer.dense((34*34*2), 512)\n","        self.fc2   = slayer.dense(512, 10)\n","\n","    def forward(self, spikeInput):\n","        spike = self.slayer.spikeLoihi(self.fc1(spikeInput))\n","        spike = self.slayer.delayShift(spike, 1)\n","        # A minimum axonal delay of 1 tick is required in Loihi hardare\n","        spike = self.slayer.spikeLoihi(self.fc2(spike))\n","        spike = self.slayer.delayShift(spike, 1)\n","        return spike\n","\n","def save_ckp(state, is_best_loss, is_best_acc, checkpoint_dir, best_model_dir):\n","    f_path = checkpoint_dir+'checkmnist.pt'\n","    torch.save(state, f_path)\n","    if is_best_loss:\n","        best_fpath = best_model_dir+'bestmnist.pt'\n","        shutil.copyfile(f_path, best_fpath)\n","    if is_best_acc:\n","        acc_fpath =  best_model_dir+'best_acc_mnist.pt'\n","        shutil.copyfile(f_path, acc_fpath)\n","        \n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']\n","\n","if __name__ == '__main__':\n","\t# Define the cuda device to run the code on.\n","\tdevice = torch.device('cuda')\n","\n","\t# Create network instance.\n","\tnet = Network(netParams).to(device)\n","\n","\t# Create snn loss instance.\n","\terror = snn.loss(netParams, snn.loihi).to(device)\n","\n","\t# Define optimizer module.\n","\toptimizer = snn.utils.optim.Nadam(net.parameters(), lr = 0.01, amsgrad = True)\n"," \n","  # Learning stats instance.\n","  stats = snn.utils.stats()\n","      \n","  ckp_path = \"/content/drive/My Drive/checkpoint/best_acc_mnist.pt\"\n","  net, optimizer, start_epoch = load_ckp(ckp_path, net, optimizer)  \n","\n","\n","\t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQj_1hGc61cx"},"source":["#Gradient Based Attack\n"]},{"cell_type":"code","metadata":{"id":"3jBg6beuIB6L"},"source":["testingSet = nmnistDataset(datasetPath  =  '/content/NMNIST/Test/', \n","                              sampleFile  ='/content/NMNIST/Test/test.txt',\n","                              samplingTime=1.0,\n","                              sampleLength=350)\n","testLoader = DataLoader(dataset=testingSet, batch_size=1, shuffle=False, num_workers=4)\n","\n","import argparse\n","import numpy as np\n","import pdb \n","import torch, torchvision\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","def update(x,y,t,s,A):\n","  for i in range(x-s,x+s+1):\n","     for j in range(y-s,y+s+1):\n","        if not(i==x and j==y) and i>0 and j>0 and i<34 and j<34:\n","\t        A[i][j] = t\n","    \n","\n","#Select the device to run the code \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print ('Device: ' + str(device))\n","\n","def calc_gradients(\n","\t\ttest_dataloader,\n","\t\tmodel,\n","\t\tmax_iter,\n","\t\tlearning_rate,\n","    samples,\n","\t\tfilter_on,\n","\t\ts,\n","\t\tT,\n","\t\ttargets=None,\n","\t\tweight_loss2=1,\n","\t\tbatch_size=1,\n","\t\tseq_len=40,):\n","\t\n","  min_loss = 1e-5\n","  prev_loss = 1e-5\n","  correct = np.zeros(max_iter)\t\n","  for batch_index, (input_image, input_target, input_label) in enumerate(test_dataloader):\n","\n","    #Define the modifier and the optimizer\n","    modif = torch.Tensor(1, seq_len, 2, 34, 34).fill_(1).to(device)\n","    modifier = torch.nn.Parameter(modif, requires_grad=True)\n","    optimizer = torch.optim.Adam([modifier], lr=learning_rate)\n","\n","\n","    #if batch_index>samples:\n","     # continue\n","    model.eval()\n","    with torch.no_grad():\t\n","      input_image, input_label = input_image.to(device), input_label.to(device)\n","\n","    #Clean video prediction \n","    print(f'Batch Number: {batch_index}/{len(test_dataloader)}')\n","    print('------------------prediction for clean video-------------------')\n","    input_image = Variable(input_image, requires_grad=True)\n","    output = model.forward(torch.reshape(input_image, (1,2312,1,1,350)))\n","    pre_label= snn.predict.getClass(output)\n","    p = torch.nn.Softmax(dim=1)\t\t\n","\n","    print (f'Prediction: {pre_label}, Original_label: {input_label.cpu().numpy()}')\n","\n","    print('------------------prediction for adversarial video-------------------')\n","\n","    min_in = input_image.min().detach() #0\n","    max_in = input_image.max().detach() #1\n","    \n","    all_loss = []\n","    \n","    for iiter in range(max_iter):\n","      with open ('/content/modifier.txt','w') as file:\n","        file.write(str(modifier))      \n","      print(modifier[0,0,0,0,0])\t\t\t\n","      input_image = Variable(input_image, requires_grad=True)\n","      #model.lstm.reset_hidden_state()\n","      \n","      #Frames to be perturbed\n","      indicator = [0]*350\n","      for i,x in enumerate(indicator):\n","        if (i>95 and i<125) or (i>175 and i<210) or (i>290 and i<310 ):\n","          x=1\n","    \n","      #Perturbating the frames\n","\n","      input_image=torch.reshape(input_image, (1,350,2,34,34))\n","      true_image = torch.clamp((modifier[0,0,:,:,:]+input_image[0,0,:,:,:]), min_in, max_in)\n","      true_image = torch.unsqueeze(true_image, 0)\n","      \n","      for ll in range(seq_len-1): #seq_len =1450\n","        if indicator[ll+1] == 1:\n","          mask_temp = torch.clamp((modifier[0,ll+1,:,:,:]+input_image[0,ll+1,:,:,:]), min_in, max_in)\n","        else:\n","          mask_temp = input_image[0,ll+1,:,:,:]\n","        mask_temp = torch.unsqueeze(mask_temp,0)\n","        true_image = torch.cat((true_image, mask_temp),0)\n","\n","      #######\t\t Filter section \t######\n","      if filter_on :\n","        img = torch.reshape(true_image, (2,34,34,350)).cpu().detach()\n","        TD = snn.io.spikeArrayToEvent(img.numpy(), samplingTime=1)\n","        snn.io.encodeNpSpikes(\"/content/drive/My Drive/temp/img.npy\", TD)\n","        data= np.load('/content/drive/My Drive/temp/img.npy'.format(i))\n","        data= data[data[:,3].argsort()] #sort elements by timestamp\n","        temp=np.zeros((34,34))\n","        real=[]\n","        for d in data:\n","          update(int(d[0]),int(d[1]),d[3],s,temp)\n","          if d[3]-temp[int(d[0])][int(d[1])]<T:\n","            real.append(d)\n","        real=np.stack(real, axis=0 )\n","        with open(\"/content/drive/My Drive/temp/fil.npy\", \"wb\") as f:\n","          np.save(f,real)\n","        true_image = snn.io.readNpSpikes(\n","            \"/content/drive/My Drive/temp/fil.npy\"\n","            ).toSpikeTensor(torch.zeros((2,34,34,350)),\n","            samplingTime=1.0)\n","        true_image =  true_image.to(device)\n","      true_image = torch.reshape(true_image, (1,350,2,34,34))\t\t\t\n","      #######\t\t End Filter Section \t #######\t\t\t\t\t\t\t\n","     \n","      #Prediction on the adversarial video\n","      output = model.forward(torch.reshape(true_image, (1,2312,1,1,350)))\n","      pre_label = snn.predict.getClass(output)\n","      numSpikes = torch.sum(output, 4, keepdim=True)\n","      numSpikes= torch.div(numSpikes,torch.max(numSpikes))\t\n","      probs= p(numSpikes.reshape((numSpikes.shape[0], -1)))\t \n","      #extracting the probability of true label \n","      zero_array = torch.zeros(10).to(device) ## 1450 invece di 11 e probs=p(output)\n","      zero_array[input_label.cpu()] = 1\n","      true_label_onehot = probs*zero_array\n","      true_label_prob = torch.sum(true_label_onehot, 1)\n","\n","      if pre_label.numpy()==input_label.cpu().numpy() : correct[iiter]+=1\n","\t\n","      #Loss\n","      if targets is None:\n","        loss1 = -torch.log(1 - true_label_prob + 1e-6)\n","      else:\n","        loss1 = -torch.log(true_label_prob + 1e-6)\n","      loss1 = torch.mean(loss1)\n","      input_image=torch.reshape(input_image, (1,350,2,34,34))\n","      loss2 = torch.sum(torch.sqrt(torch.mean(torch.pow((true_image-input_image+1e-6), 2), dim=0).mean(dim=2).mean(dim=2).mean(dim=1)))\n","\n","      loss = loss1 + weight_loss2 * loss2\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      if True: #iiter % max_iter == 0: \n","        print (f'Probability for ground truth label : {true_label_prob.detach().cpu().numpy()}')\n","        if prev_loss < loss : \n","          print(f'Iteration: [{iiter}/{max_iter}], Loss: {loss}(\\u25b2), Loss1: {loss1}, Loss2: {loss2}')\n","        elif prev_loss > loss: \n","          print(f'Iteration: [{iiter}/{max_iter}], Loss: {loss}(\\u25bc), Loss1: {loss1}, Loss2: {loss2}')\n","        else: \n","          print(f'Iteration: [{iiter}/{max_iter}], Loss: {loss}, Loss1: {loss1}, Loss2: {loss2}')\n","      prev_loss = loss\n","\n","      break_condition = False\n","      if loss < min_loss:\n","        if torch.abs(loss-min_loss) < 0.0001:\n","            break_condition = True\n","            print ('Aborting early!')\n","        min_loss = loss\n","\n","      # Empty cache\n","      if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","    # Save adversarial dataset as spike events\n","    image = torch.reshape(true_image, (2,34,34,350)).cpu().detach()\n","    TD = snn.io.spikeArrayToEvent(image.numpy(), samplingTime=1)\n","    snn.io.encodeNpSpikes(\"/content/adversarial/{}.npy\".format(batch_index), TD)\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\t\n","  print(correct)\n","\n","\n","def main(net):\n","\n","  args = {\n","      'description': 'Sparse Adversarial Perturbations',\n","      'num_iter': 20,\n","      'learning_rate': 100,\n","      'samples':11,\n","\t\t\t'filter_on': False,\n","\t\t\t's': 2,\n","\t\t\t'T': 5,\n","      'target': None,\n","      'weight_loss2': 0,\n","      'split_path': None,\n","      'split_number': 1,\n","      'img_dim': 34,\n","      'channels': 2,\n","\t\t\t}\n","  seq_len = 350 #number of frames in a video\n","  batch_size = 1\n","  targets = None\n","  image_shape = (args['channels'], args['img_dim'], args['img_dim'])\n","  net = net.to(device)\n","  net.train()\n","  print('Model Loaded Successfully!')\n","\t#Call the function to generate the adversarial videos\n","  calc_gradients(\n","\ttestLoader,\n","\tnet,\n","\targs['num_iter'],\n","\targs['learning_rate'],\n","  args['samples'],\n","\targs['filter_on'],\n","\targs['s'],\n","\targs['T'],\n","\ttargets,\n","\targs['weight_loss2'],\n","\tbatch_size,\n","\tseq_len,\n","\t)\n","\n","if __name__ == '__main__':\n","\tmain(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YpDnqcJzEqKV"},"source":["# BAF \n","Several BAFs applied to the adversarial examples and tested. \n"]},{"cell_type":"code","metadata":{"id":"29LYP8C74EiM"},"source":["import numpy as np\n","\n","def update(x,y,t,A,s):\n","  for i in range(x-s,x+s+1):\n","     for j in range(y-s,y+s+1):\n","        if not(i==x and j==y) and i>0 and j>0 and i<34 and j<34:\n","\t        A[i][j] = t\n","testSet = nmnistDataset(datasetPath  =  '/content/adversarial/', \n","                              sampleFile  ='/content/NMNIST/Test/test.txt',\n","                              samplingTime=1.0,\n","                              sampleLength=350)\n","testLoader = DataLoader(dataset=testSet, batch_size=1, shuffle=False, num_workers=4)\n","       \n","for s in [1,2,3,4]:\n","  stats1 = snn.utils.stats()\n","  stats5 = snn.utils.stats()\n","  stats10 = snn.utils.stats()\n","  stats20 = snn.utils.stats()      \n","  for i in range(100):     \n","    input,target,label = testSet[i]    \n","    data= np.load('/content/NMNIST/Test/{}.npy'.format(i))\n","    data= data[data[:,3].argsort()] #sort elements by timestamp\n","    temp=np.zeros((34,34))\n","    real1=[]\n","    real5=[]\n","    real10=[]\n","    real20=[]\n","    for d in data:\n","      update(int(d[0]),int(d[1]),d[3],temp,s)\n","      if d[3]-temp[int(d[0])][int(d[1])]<2:\n","        real1.append(d)\n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<8:\n","        real5.append(d)   \n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<15:\n","        real10.append(d)\n","\n","      if d[3]-temp[int(d[0])][int(d[1])]<25:\n","        real20.append(d)  \n","\n","    real1=np.stack(real1, axis=0 )\n","    with open(\"/content/filtro/1.npy\", \"wb\") as f:\n","      np.save(f,real1)\n","    input1 = snn.io.readNpSpikes(\n","        \"/content/filtro/1.npy\").toSpikeTensor(torch.zeros((2,34,34,350)), samplingTime=1.0)\n","    input1 =  input1.to(device)\n","    output1 = net.forward(torch.reshape(input1, (1,2312,1,1,350)))\n","    stats1.testing.correctSamples += torch.sum( snn.predict.getClass(output1) == label ).data.item()\n","    stats1.testing.numSamples     += 1\n","    #stats1.print(1,i)\n","\n","    real5=np.stack(real5, axis=0 )\n","    with open(\"/content/filtro/5.npy\", \"wb\") as f:\n","      np.save(f,real5)\n","    input5 = snn.io.readNpSpikes(\n","        \"/content/filtro/5.npy\").toSpikeTensor(torch.zeros((2,34,34,350)), samplingTime=1.0)\n","    input5 =  input5.to(device)\n","    output5 = net.forward(torch.reshape(input5, (1,2312,1,1,350)))\n","    stats5.testing.correctSamples += torch.sum( snn.predict.getClass(output5) == label ).data.item()\n","    stats5.testing.numSamples     += 1\n","    #stats5.print(1,i)\n","\n","    real10=np.stack(real10, axis=0 )\n","    with open(\"/content/filtro/10.npy\", \"wb\") as f:\n","      np.save(f,real10)\n","    input10 = snn.io.readNpSpikes(\n","        \"/content/filtro/10.npy\").toSpikeTensor(torch.zeros((2,34,34,350)), samplingTime=1.0)\n","    input10 =  input10.to(device)\n","    output10 = net.forward(torch.reshape(input10, (1,2312,1,1,350)))\n","    stats10.testing.correctSamples += torch.sum( snn.predict.getClass(output10) == label ).data.item()\n","    stats10.testing.numSamples     += 1\n","    #stats10.print(1,i)    \n","\n","    real20=np.stack(real20, axis=0 )\n","    with open(\"/content/filtro/20.npy\", \"wb\") as f:\n","      np.save(f,real20)\n","    input20= snn.io.readNpSpikes(\n","        \"/content/filtro/20.npy\").toSpikeTensor(torch.zeros((2,34,34,350)), samplingTime=1.0)\n","    input20 =  input20.to(device)\n","    output20 = net.forward(torch.reshape(input20, (1,2312,1,1,350)))\n","    stats20.testing.correctSamples += torch.sum( snn.predict.getClass(output20) == label ).data.item()\n","    stats20.testing.numSamples     += 1\n","\n","  stats1.print(1,i)\n","  stats5.print(1,i)\n","  stats10.print(1,i)\n","  stats20.print(1,i)"],"execution_count":null,"outputs":[]}]}